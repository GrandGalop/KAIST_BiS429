{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def assign_tensor(arr, device):\n",
    "    return torch.from_numpy(arr).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "cf = AttrDict() #cf = {}\n",
    "\n",
    "cf.n_epochs = 100\n",
    "cf.size_of_batch = 128\n",
    "cf.data_size = None\n",
    "\n",
    "cf.apply_inv = True\n",
    "cf.label_scale = 0.94\n",
    "cf.img_scale = 1.0\n",
    "cf.apply_scaling = True    \n",
    "\n",
    "\n",
    "cf.numperceptrons = [784, 500, 500, 10]\n",
    "cf.var_out = 1\n",
    "cf.activation_function = F.TANH\n",
    "cf.numlayers = len(cf.numperceptrons)    \n",
    "cf.variance = torch.ones(cf.numlayers)\n",
    "\n",
    "cf.inference_beta_parameter = 0.1\n",
    "cf.max_iterations = 50\n",
    "cf.threshold_option = 1e-6\n",
    "\n",
    "# optim parameters\n",
    "cf.lr = 1e-3\n",
    "cf.type_of_optimzer = \"ADAM\"\n",
    "cf.adam_beta_parameter_1 = 0.9\n",
    "cf.adam_beta_parameter_2 = 0.999\n",
    "cf.epsilon = 1e-8\n",
    "cf.decay_r = 0.9    \n",
    "\n",
    "cf.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "'''\n",
    "\n",
    "class NetworkForPredictiveCoding(object):\n",
    "    def __init__(self, cf):\n",
    "\n",
    "        self.device = cf.device\n",
    "        self.activation_function = cf.activation_function\n",
    "        \n",
    "        self.adam_beta_parameter_1 = cf.adam_beta_parameter_1\n",
    "        self.adam_beta_parameter_2 = cf.adam_beta_parameter_2\n",
    "        self.inference_beta_parameter = cf.inference_beta_parameter\n",
    "        self.lr = cf.lr\n",
    "        self.threshold_option = cf.threshold_option / (sum(cf.numperceptrons) - cf.numperceptrons[0])\n",
    "        \n",
    "        self.numlayers = cf.numlayers\n",
    "        self.numperceptrons = cf.numperceptrons\n",
    "        self.variance= cf.variance.float().to(self.device)\n",
    "        self.max_iterations = cf.max_iterations\n",
    "        self.size_of_batch = cf.size_of_batch\n",
    "        \n",
    "        self.type_of_optimzer = cf.type_of_optimzer\n",
    "        self.epsilon = cf.epsilon\n",
    "        self.decay_r = cf.decay_r\n",
    "        self.bias_constant = [[] for _ in range(self.numlayers)]\n",
    "        self.weight_constant = [[] for _ in range(self.numlayers)]\n",
    "        self.bias_value = [[] for _ in range(self.numlayers)]\n",
    "        self.weight_value = [[] for _ in range(self.numlayers)]\n",
    "\n",
    "        self.Weight = None\n",
    "        self.bias = None\n",
    "        self._parameter_intialization()\n",
    "        \n",
    "    def _parameter_intialization(self):\n",
    "        bias = [[] for _ in range(self.numlayers)] # bias = [[], [], [] ..., []]\n",
    "        weights = [[] for _ in range(self.numlayers)] # weights = [[], [], [] ..., []]\n",
    "\n",
    "        for l in range(self.numlayers - 1):\n",
    "            norm_b = 0\n",
    "            if self.activation_function == F.TANH:\n",
    "                weight_normalization = np.sqrt(6 / (self.numperceptrons[l + 1] + self.numperceptrons[l]))            \n",
    "            else:\n",
    "                raise ValueError(f\"{self.activation_function} not supported\")\n",
    "\n",
    "            weight_layer = np.random.uniform(-1, 1, size=(self.numperceptrons[l + 1], self.numperceptrons[l])) * weight_normalization\n",
    "            bias_layer = np.zeros((self.numperceptrons[l + 1], 1)) + norm_b * np.ones((self.numperceptrons[l + 1], 1))\n",
    "            weights[l] = assign_tensor(weight_layer, self.device)\n",
    "            bias[l] = assign_tensor(bias_layer, self.device)\n",
    "\n",
    "        self.Weight = weights #  Weight[layer number] [unit number of l+1 layer]\n",
    "        self.bias = bias\n",
    "        for l in range(self.numlayers - 1):\n",
    "            self.bias_constant[l] = torch.zeros_like(self.bias[l])\n",
    "            self.weight_constant[l] = torch.zeros_like(self.Weight[l])\n",
    "            self.bias_value[l] = torch.zeros_like(self.bias[l])\n",
    "            self.weight_value[l] = torch.zeros_like(self.Weight[l])\n",
    "    \n",
    "    def _gradient_updates(self, weight_gradient, bias_gradient, number_epoch=None, batches_number=None, current_batch=None):\n",
    "\n",
    "        if self.type_of_optimzer == \"ADAM\":\n",
    "            for l in range(self.numlayers - 1):\n",
    "                bias_gradient[l] = bias_gradient[l].unsqueeze(dim=1)\n",
    "                self.bias_constant[l] = self.adam_beta_parameter_1 * self.bias_constant[l] + (1 - self.adam_beta_parameter_1) * bias_gradient[l]\n",
    "\n",
    "\n",
    "                self.weight_constant[l] = self.adam_beta_parameter_1 * self.weight_constant[l] + (1 - self.adam_beta_parameter_1) * weight_gradient[l]\n",
    "\n",
    "                self.bias_value[l] = self.adam_beta_parameter_2 * self.bias_value[l] + (1 - self.adam_beta_parameter_2) * bias_gradient[l] ** 2\n",
    "                self.weight_value[l] = self.adam_beta_parameter_2 * self.weight_value[l] + (1 - self.adam_beta_parameter_2) * weight_gradient[l] ** 2\n",
    "\n",
    "                self.Weight[l] = self.Weight[l] + self.lr * np.sqrt(1 - self.adam_beta_parameter_2 ** ((number_epoch) * batches_number + current_batch)) * self.weight_constant[l] / (\n",
    "                    torch.sqrt(self.weight_value[l]) + self.epsilon\n",
    "                )\n",
    "                self.bias[l] = self.bias[l] + self.lr* np.sqrt(1 - self.adam_beta_parameter_2 ** ((number_epoch) * batches_number + current_batch)) * self.bias_constant[l] / (\n",
    "                    torch.sqrt(self.bias_value[l]) + self.epsilon\n",
    "                )\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"{self.type_of_optimzer} not supported\")\n",
    "\n",
    "    def inference(self, Neuronal_state_array, size_of_batch):\n",
    "        # print(size_of_batch)\n",
    "        error = [[] for _ in range(self.numlayers)]\n",
    "        Neuronal_output_array = [[] for _ in range(self.numlayers)]\n",
    "        Neuronal_derivate_array = [[] for _ in range(self.numlayers)]\n",
    "        previous_entropy = 0\n",
    "        stop_iteration = 0\n",
    "        inference_beta_parameter = self.inference_beta_parameter\n",
    "\n",
    "        for l in range(1, self.numlayers):\n",
    "            Neuronal_output_layer = F.f(Neuronal_state_array[l - 1], self.activation_function) # Neuronal output from neurons = f(x^(l+1)_i)\n",
    "            Neuronal_derivate_layer = F.f_deriv(Neuronal_state_array[l - 1], self.activation_function) # Derivative of Neuronal output from neurons = f'(x^(l+1)_i)\n",
    "            Neuronal_output_array[l - 1] = Neuronal_output_layer \n",
    "            Neuronal_derivate_array[l - 1] = Neuronal_derivate_layer\n",
    "\n",
    "            bias = self.bias[l - 1].repeat(1, size_of_batch)\n",
    "            \n",
    "            # In the Predictive Coding Model - Inference, Use the equation to calcuate 'epsilon'.\n",
    "            # It will cacluate the errors in the lth neuronal layer\n",
    "            # It is the initialization step of error before the actual inference\n",
    "            # for i in range(0, len(self.Weight[l-1])):\n",
    "            #     mu.append(self.Weight[l-1][i] @ Neuronal_output_array[l-1])\n",
    "            mu = self.Weight[l-1] @ Neuronal_output_array[l-1] + bias\n",
    "            error[l] = (Neuronal_state_array[l] - mu) / self.variance[l]\n",
    "\n",
    "            # In the Predictive Coding Model - Inference, Use the equation to calcuate 'F(previous entropy)'\n",
    "            # Since entropy is summed over whole layers, entropy should be accumulated at every layers\n",
    "            # Caution : entropy is not just summation of error^2!\n",
    "            # It is the intialization step of entropy before actual inference\n",
    "            previous_entropy = previous_entropy + (-1/2) * torch.sum((Neuronal_state_array[l] - mu)**2 / self.variance[l])\n",
    "\n",
    "        for iterations in range(self.max_iterations):            \n",
    "            for l in range(1, self.numlayers - 1):   \n",
    "\n",
    "                # Neuronal_state (x) derivations are calculated based on the errors and the weights in the Predictive Coding Model-inference\n",
    "                # When we know the derivation, we can update neruonal states using their derivations\n",
    "                Neuronal_state_array[l] = Neuronal_state_array[l] + inference_beta_parameter * ((-error[l]) + Neuronal_derivate_array[l] * (torch.transpose(self.Weight[l], 0, 1) @ error[l+1]))\n",
    "            \n",
    "            current_entropy = 0\n",
    "            for l in range(1, self.numlayers):\n",
    "                Neuronal_output_layer = F.f(Neuronal_state_array[l - 1], self.activation_function)\n",
    "                Neuronal_derivate_layer = F.f_deriv(Neuronal_state_array[l - 1], self.activation_function)\n",
    "                Neuronal_output_array[l - 1] = Neuronal_output_layer\n",
    "                Neuronal_derivate_array[l - 1] = Neuronal_derivate_layer\n",
    "\n",
    "                # In the Predictive Coding Model - Inference, Use the equation to calcuate 'epsilon'.\n",
    "                # It will cacluate the errors in the lth neuronal layer\n",
    "                # It is the update step of error before the actual inference\n",
    "                bias = self.bias[l - 1].repeat(1, size_of_batch)\n",
    "                mu = self.Weight[l-1] @ Neuronal_output_layer + bias\n",
    "                error[l] = (Neuronal_state_array[l] - mu) / self.variance[l]\n",
    "\n",
    "\n",
    "                # In the Predictive Coding Model - Inference, Use the equation to calcuate 'F(previous entropy)'\n",
    "                # Since entropy is summed over whole layers, entropy should be accumulated at every layers\n",
    "                # Caution : entropy is not just summation of error^2!\n",
    "                # It is the udpate step of entropy before actual inference\n",
    "                current_entropy = current_entropy + (-1/2) * torch.sum((Neuronal_state_array[l] - mu)**2 / self.variance[l])\n",
    "\n",
    "            entropy_difference = current_entropy - previous_entropy\n",
    "            threshold = self.threshold_option  * self.inference_beta_parameter / self.variance[self.numlayers - 1]\n",
    "            if torch.any(entropy_difference < 0):\n",
    "                inference_beta_parameter = inference_beta_parameter / 2\n",
    "            elif torch.mean(entropy_difference) < threshold:\n",
    "                break\n",
    "\n",
    "            previous_entropy = current_entropy\n",
    "            stop_iteration = iterations\n",
    "\n",
    "        return Neuronal_state_array, error, stop_iteration\n",
    "\n",
    "    def parameters_update(self, Neuronal_state_array, error, size_of_batch, number_epoch=None, batches_number=None, current_batch=None):\n",
    "        # print(size_of_batch)\n",
    "        # print()\n",
    "        weight_gradient = [[] for _ in range(self.numlayers - 1)]\n",
    "        bias_gradient = [[] for _ in range(self.numlayers - 1)]\n",
    "\n",
    "        for l in range(self.numlayers - 1):\n",
    "            # It is 'Predictive Coding Model - Update' part\n",
    "            # Using the 'error' array and the equation, calculate the gradient of weight (a.k.a derivate of weight)\n",
    "\n",
    "            weight_gradient[l] = error[l+1]  @ torch.transpose(F.f(Neuronal_state_array[l], self.activation_function), 0, 1)\n",
    "\n",
    "            # Using the 'error' array and the equation, calculate the gradient of bias (a.k.a derivate of bias)\n",
    "            bias_gradient[l] = ((error[l+1] @ torch.ones(size_of_batch,1).cuda()) / self.size_of_batch).squeeze()\n",
    "\n",
    "            # Below step was not noticed in the slide, but since we do the batch update, we need this normalization step\n",
    "            weight_gradient[l] = self.variance[-1]*(1/size_of_batch)*weight_gradient[l]\n",
    "            bias_gradient[l] = self.variance[-1]*(1/size_of_batch)*bias_gradient[l]\n",
    "            \n",
    "            \n",
    "\n",
    "        self._gradient_updates(weight_gradient, bias_gradient, number_epoch=number_epoch, batches_number=batches_number, current_batch=current_batch)    \n",
    "    \n",
    "            \n",
    "    def epoch_for_train(self, x_batches, y_batches, number_epoch=None):\n",
    "        batches_number = len(x_batches)\n",
    "        for batch_id, (x_batch, y_batch) in enumerate(zip(x_batches, y_batches)):\n",
    "\n",
    "            if batch_id % 500 == 0 and batch_id > 0:\n",
    "                print(f\"batch {batch_id}\")\n",
    "\n",
    "            x_batch = assign_tensor(x_batch, self.device)\n",
    "            y_batch = assign_tensor(y_batch, self.device)\n",
    "            size_of_batch = x_batch.size(1)\n",
    "\n",
    "            Neuronal_state_array = [[] for _ in range(self.numlayers)]\n",
    "            Neuronal_state_array[0] = x_batch\n",
    "            for l in range(1, self.numlayers):\n",
    "                bias = self.bias[l - 1].repeat(1, size_of_batch)\n",
    "                Neuronal_state_array[l] = self.Weight[l - 1] @ F.f(Neuronal_state_array[l - 1], self.activation_function) + bias\n",
    "            Neuronal_state_array[self.numlayers - 1] = y_batch\n",
    "\n",
    "            Neuronal_state_array, error, _ = self.inference(Neuronal_state_array, size_of_batch)\n",
    "            self.parameters_update(\n",
    "                Neuronal_state_array, error, size_of_batch, number_epoch=number_epoch, batches_number=batches_number, current_batch=batch_id\n",
    "            )\n",
    "            \n",
    "    def epoch_for_test(self, x_batches, y_batches):\n",
    "        accuracy_sets = []\n",
    "        for x_batch, y_batch in zip(x_batches, y_batches):\n",
    "            x_batch = assign_tensor(x_batch, self.device)\n",
    "            y_batch = assign_tensor(y_batch, self.device)\n",
    "            size_of_batch = x_batch.size(1)\n",
    "\n",
    "            Neuronal_state_array = [[] for _ in range(self.numlayers)]\n",
    "            Neuronal_state_array[0] = x_batch\n",
    "            for l in range(1, self.numlayers):\n",
    "                bias = self.bias[l - 1].repeat(1, size_of_batch)\n",
    "                Neuronal_state_array[l] = self.Weight[l - 1] @ F.f(Neuronal_state_array[l - 1], self.activation_function) + bias\n",
    "            y_prediction = Neuronal_state_array[-1]\n",
    "\n",
    "            accuracy = accuracy_for_MNIST(y_prediction, y_batch)\n",
    "            accuracy_sets.append(accuracy)\n",
    "        return accuracy_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mnist_train_set():\n",
    "    #return torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=True)\n",
    "    return torchvision.datasets.MNIST(\"MNIST_train\", download=True, train=True)\n",
    "\n",
    "\n",
    "def get_mnist_test_set():\n",
    "    return torchvision.datasets.MNIST(\"MNIST_test\", download=True, train=False)\n",
    "\n",
    "\n",
    "def onehot(label, n_classes=10):\n",
    "    arr = np.zeros([10])\n",
    "    arr[int(label)] = 1.0\n",
    "    return arr\n",
    "\n",
    "\n",
    "def img_to_np(img):\n",
    "    return np.array(img).reshape([784]) / 255.0\n",
    "\n",
    "\n",
    "def get_imgs(dataset):\n",
    "    imgs = np.array([img_to_np(dataset[i][0]) for i in range(len(dataset))])\n",
    "    return np.swapaxes(imgs, 0, 1)\n",
    "\n",
    "\n",
    "def get_labels(dataset):\n",
    "    labels = np.array([onehot(dataset[i][1]) for i in range(len(dataset))])\n",
    "    return np.swapaxes(labels, 0, 1)\n",
    "\n",
    "\n",
    "def scale_imgs(imgs, scale_factor):\n",
    "    return imgs * scale_factor + 0.5 * (1 - scale_factor) * np.ones(imgs.shape)\n",
    "\n",
    "\n",
    "def scale_labels(labels, scale_factor):\n",
    "    return labels * scale_factor + 0.5 * (1 - scale_factor) * np.ones(labels.shape)\n",
    "\n",
    "\n",
    "def accuracy_for_MNIST(pred_labels, labels):\n",
    "    correct = 0\n",
    "    size_of_batch = pred_labels.size(1)\n",
    "    for b in range(size_of_batch):\n",
    "        if torch.argmax(pred_labels[:, b]) == torch.argmax(labels[:, b]):\n",
    "            correct += 1\n",
    "    return correct / size_of_batch\n",
    "\n",
    "\n",
    "def get_batches(imgs, labels, size_of_batch):\n",
    "    n_data = imgs.shape[1]\n",
    "    n_batches = int(np.ceil(n_data / size_of_batch))\n",
    "\n",
    "    img_batches = [[] for _ in range(n_batches)]\n",
    "    label_batches = [[] for _ in range(n_batches)]\n",
    "\n",
    "    for batch in range(n_batches):\n",
    "        if batch == n_batches - 1:\n",
    "            start = batch * size_of_batch\n",
    "            img_batches[batch] = imgs[:, start:]\n",
    "            label_batches[batch] = labels[:, start:]\n",
    "        else:\n",
    "            start = batch * size_of_batch\n",
    "            end = (batch + 1) * size_of_batch\n",
    "            img_batches[batch] = imgs[:, start:end]\n",
    "            label_batches[batch] = labels[:, start:end]\n",
    "\n",
    "    return img_batches, label_batches\n",
    "\n",
    "\n",
    "def plot_imgs(img_batch, save_path):\n",
    "    img_batch = img_batch.detach().cpu().numpy()\n",
    "    size_of_batch = img_batch.shape[1]\n",
    "    dim = nearest_square(size_of_batch)\n",
    "\n",
    "    imgs = [np.reshape(img_batch[:, i], [28, 28]) for i in range(dim ** 2)]\n",
    "    _, axes = plt.subplots(dim, dim)\n",
    "    axes = axes.flatten()\n",
    "    for i, img in enumerate(imgs):\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_axis_off()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close('all')\n",
    "\n",
    "\n",
    "def nearest_square(limit):\n",
    "    answer = 0\n",
    "    while (answer + 1) ** 2 < limit:\n",
    "        answer += 1\n",
    "    return answer\n",
    "\n",
    "class AttrDict(dict):\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __getattr__ = dict.__getitem__ \n",
    "\n",
    "def main(cf):\n",
    "    print(f\"device [{cf.device}]\")\n",
    "    print(\"loading MNIST data...\")\n",
    "    train_set = get_mnist_train_set()\n",
    "    test_set = get_mnist_test_set()\n",
    "\n",
    "    img_train = get_imgs(train_set)\n",
    "    img_test = get_imgs(test_set)\n",
    "    label_train = get_labels(train_set)\n",
    "    label_test = get_labels(test_set)\n",
    "\n",
    "    if cf.data_size is not None:\n",
    "        test_size = cf.data_size // 5\n",
    "        img_train = img_train[:, 0 : cf.data_size]\n",
    "        label_train = label_train[:, 0 : cf.data_size]\n",
    "        img_test = img_test[:, 0:test_size]\n",
    "        label_test = label_test[:, 0:test_size]\n",
    "\n",
    "    msg = \"img_train {} img_test {} label_train {} label_test {}\"\n",
    "    print(msg.format(img_train.shape, img_test.shape, label_train.shape, label_test.shape))\n",
    "\n",
    "    print(\"performing preprocessing...\")\n",
    "    if cf.apply_scaling:\n",
    "        img_train = scale_imgs(img_train, cf.img_scale)\n",
    "        img_test = scale_imgs(img_test, cf.img_scale)\n",
    "        label_train = scale_labels(label_train, cf.label_scale)\n",
    "        label_test = scale_labels(label_test, cf.label_scale)\n",
    "\n",
    "    if cf.apply_inv:\n",
    "        img_train = F.f_inv(img_train, cf.activation_function)\n",
    "        img_test = F.f_inv(img_test, cf.activation_function)\n",
    "\n",
    "    model = NetworkForPredictiveCoding(cf)\n",
    "    average_accuracy = []\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(cf.n_epochs):\n",
    "            print(f\"\\nepoch {epoch}\")\n",
    "\n",
    "            img_batches, label_batches = get_batches(img_train, label_train, cf.size_of_batch)\n",
    "            print(f\"training on {len(img_batches)} batches of size {cf.size_of_batch}\")\n",
    "            model.epoch_for_train(img_batches, label_batches, number_epoch=epoch)\n",
    "\n",
    "            img_batches, label_batches = get_batches(img_test, label_test, cf.size_of_batch)\n",
    "            print(f\"testing on {len(img_batches)} batches of size {cf.size_of_batch}\")\n",
    "            accuracy_sets = model.epoch_for_test(img_batches, label_batches)\n",
    "            print(f\"average accuracy {np.mean(np.array(accuracy_sets))}\")\n",
    "            average_accuracy.append(np.mean(np.array(accuracy_sets)).item())\n",
    "            perm = np.random.permutation(img_train.shape[1])\n",
    "            img_train = img_train[:, perm]\n",
    "            label_train = label_train[:, perm]\n",
    "    plt.figure()\n",
    "    plt.plot(range(0, cf.n_epochs), average_accuracy)\n",
    "    plt.title(\"Accuracy per Epochs\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device [cuda]\n",
      "loading MNIST data...\n",
      "img_train (784, 60000) img_test (784, 10000) label_train (10, 60000) label_test (10, 10000)\n",
      "performing preprocessing...\n",
      "\n",
      "epoch 0\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.5360957278481012\n",
      "\n",
      "epoch 1\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9140625\n",
      "\n",
      "epoch 2\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9186115506329114\n",
      "\n",
      "epoch 3\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9065466772151899\n",
      "\n",
      "epoch 4\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.914754746835443\n",
      "\n",
      "epoch 5\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9156447784810127\n",
      "\n",
      "epoch 6\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.939873417721519\n",
      "\n",
      "epoch 7\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9452136075949367\n",
      "\n",
      "epoch 8\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.889932753164557\n",
      "\n",
      "epoch 9\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9491693037974683\n",
      "\n",
      "epoch 10\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9475870253164557\n",
      "\n",
      "epoch 11\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9584651898734177\n",
      "\n",
      "epoch 12\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9608386075949367\n",
      "\n",
      "epoch 13\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9668710443037974\n",
      "\n",
      "epoch 14\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9628164556962026\n",
      "\n",
      "epoch 15\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9667721518987342\n",
      "\n",
      "epoch 16\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.967068829113924\n",
      "\n",
      "epoch 17\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.965684335443038\n",
      "\n",
      "epoch 18\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9689477848101266\n",
      "\n",
      "epoch 19\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9689477848101266\n",
      "\n",
      "epoch 20\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9677610759493671\n",
      "\n",
      "epoch 21\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9697389240506329\n",
      "\n",
      "epoch 22\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9690466772151899\n",
      "\n",
      "epoch 23\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9692444620253164\n",
      "\n",
      "epoch 24\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9653876582278481\n",
      "\n",
      "epoch 25\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9706289556962026\n",
      "\n",
      "epoch 26\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9698378164556962\n",
      "\n",
      "epoch 27\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9679588607594937\n",
      "\n",
      "epoch 28\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9659810126582279\n",
      "\n",
      "epoch 29\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9715189873417721\n",
      "\n",
      "epoch 30\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9675632911392406\n",
      "\n",
      "epoch 31\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9676621835443038\n",
      "\n",
      "epoch 32\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9686511075949367\n",
      "\n",
      "epoch 33\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9683544303797469\n",
      "\n",
      "epoch 34\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9691455696202531\n",
      "\n",
      "epoch 35\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9707278481012658\n",
      "\n",
      "epoch 36\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9716178797468354\n",
      "\n",
      "epoch 37\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9671677215189873\n",
      "\n",
      "epoch 38\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9698378164556962\n",
      "\n",
      "epoch 39\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9725079113924051\n",
      "\n",
      "epoch 40\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9707278481012658\n",
      "\n",
      "epoch 41\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.966376582278481\n",
      "\n",
      "epoch 42\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9690466772151899\n",
      "\n",
      "epoch 43\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.971815664556962\n",
      "\n",
      "epoch 44\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9702333860759493\n",
      "\n",
      "epoch 45\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9730023734177216\n",
      "\n",
      "epoch 46\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9714200949367089\n",
      "\n",
      "epoch 47\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.973496835443038\n",
      "\n",
      "epoch 48\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9723101265822784\n",
      "\n",
      "epoch 49\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9724090189873418\n",
      "\n",
      "epoch 50\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9710245253164557\n",
      "\n",
      "epoch 51\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9740901898734177\n",
      "\n",
      "epoch 52\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9752768987341772\n",
      "\n",
      "epoch 53\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9732990506329114\n",
      "\n",
      "epoch 54\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9729034810126582\n",
      "\n",
      "epoch 55\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9716178797468354\n",
      "\n",
      "epoch 56\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9737935126582279\n",
      "\n",
      "epoch 57\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9728045886075949\n",
      "\n",
      "epoch 58\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9732990506329114\n",
      "\n",
      "epoch 59\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9744857594936709\n",
      "\n",
      "epoch 60\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9742879746835443\n",
      "\n",
      "epoch 61\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9724090189873418\n",
      "\n",
      "epoch 62\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9732990506329114\n",
      "\n",
      "epoch 63\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9737935126582279\n",
      "\n",
      "epoch 64\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9719145569620253\n",
      "\n",
      "epoch 65\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9753757911392406\n",
      "\n",
      "epoch 66\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.973496835443038\n",
      "\n",
      "epoch 67\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9702333860759493\n",
      "\n",
      "epoch 68\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9742879746835443\n",
      "\n",
      "epoch 69\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9697389240506329\n",
      "\n",
      "epoch 70\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9719145569620253\n",
      "\n",
      "epoch 71\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9759691455696202\n",
      "\n",
      "epoch 72\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9735957278481012\n",
      "\n",
      "epoch 73\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9755735759493671\n",
      "\n",
      "epoch 74\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9759691455696202\n",
      "\n",
      "epoch 75\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9742879746835443\n",
      "\n",
      "epoch 76\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9746835443037974\n",
      "\n",
      "epoch 77\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9746835443037974\n",
      "\n",
      "epoch 78\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9710245253164557\n",
      "\n",
      "epoch 79\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9698378164556962\n",
      "\n",
      "epoch 80\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9572784810126582\n",
      "\n",
      "epoch 81\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9642009493670886\n",
      "\n",
      "epoch 82\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.962618670886076\n",
      "\n",
      "epoch 83\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.963310917721519\n",
      "\n",
      "epoch 84\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9681566455696202\n",
      "\n",
      "epoch 85\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9662776898734177\n",
      "\n",
      "epoch 86\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9702333860759493\n",
      "\n",
      "epoch 87\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9655854430379747\n",
      "\n",
      "epoch 88\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9677610759493671\n",
      "\n",
      "epoch 89\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9686511075949367\n",
      "\n",
      "epoch 90\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9683544303797469\n",
      "\n",
      "epoch 91\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9683544303797469\n",
      "\n",
      "epoch 92\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.96875\n",
      "\n",
      "epoch 93\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.966376582278481\n",
      "\n",
      "epoch 94\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.943631329113924\n",
      "\n",
      "epoch 95\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9526305379746836\n",
      "\n",
      "epoch 96\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9541139240506329\n",
      "\n",
      "epoch 97\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9614319620253164\n",
      "\n",
      "epoch 98\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9613330696202531\n",
      "\n",
      "epoch 99\n",
      "training on 469 batches of size 128\n",
      "testing on 79 batches of size 128\n",
      "average accuracy 0.9622231012658228\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cf = AttrDict() #cf = {}\n",
    "\n",
    "    cf.n_epochs = 100\n",
    "    cf.size_of_batch = 128\n",
    "    cf.data_size = None\n",
    "\n",
    "    cf.apply_inv = True\n",
    "    cf.label_scale = 0.94\n",
    "    cf.img_scale = 1.0\n",
    "    cf.apply_scaling = True    \n",
    "    \n",
    "\n",
    "    cf.numperceptrons = [784, 500, 500, 10]\n",
    "    cf.var_out = 1\n",
    "    cf.activation_function = F.TANH\n",
    "    cf.numlayers = len(cf.numperceptrons)    \n",
    "    cf.variance = torch.ones(cf.numlayers)\n",
    "\n",
    "    cf.inference_beta_parameter = 0.1\n",
    "    cf.max_iterations = 50\n",
    "    cf.threshold_option = 1e-6\n",
    "\n",
    "    # optim parameters\n",
    "    cf.lr = 1e-3\n",
    "    cf.type_of_optimzer = \"ADAM\"\n",
    "    cf.adam_beta_parameter_1 = 0.9\n",
    "    cf.adam_beta_parameter_2 = 0.999\n",
    "    cf.epsilon = 1e-8\n",
    "    cf.decay_r = 0.9    \n",
    "\n",
    "    cf.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    main(cf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = AttrDict() #cf = {}\n",
    "\n",
    "cf.n_epochs = 100\n",
    "cf.size_of_batch = 128\n",
    "cf.data_size = None\n",
    "\n",
    "cf.apply_inv = True\n",
    "cf.label_scale = 0.94\n",
    "cf.img_scale = 1.0\n",
    "cf.apply_scaling = True    \n",
    "\n",
    "\n",
    "cf.numperceptrons = [784, 500, 500, 10]\n",
    "cf.var_out = 1\n",
    "cf.activation_function = F.TANH\n",
    "cf.numlayers = len(cf.numperceptrons)    \n",
    "cf.variance = torch.ones(cf.numlayers)\n",
    "\n",
    "cf.inference_beta_parameter = 0.1\n",
    "cf.max_iterations = 50\n",
    "cf.threshold_option = 1e-6\n",
    "\n",
    "# optim parameters\n",
    "cf.lr = 1e-3\n",
    "cf.type_of_optimzer = \"ADAM\"\n",
    "cf.adam_beta_parameter_1 = 0.9\n",
    "cf.adam_beta_parameter_2 = 0.999\n",
    "cf.epsilon = 1e-8\n",
    "cf.decay_r = 0.9    \n",
    "\n",
    "cf.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
